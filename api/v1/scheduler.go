//go:build !ignore_autogenerated
// +build !ignore_autogenerated

package v1

import (
	"context"
	"fmt"
	"regexp"
	"strings"
	"sync"
)

type TaskScheduler struct {
	step    MainStep
	builder *TaskBuilder
}

func NewTaskScheduler(step MainStep) *TaskScheduler {
	return &TaskScheduler{
		step: step,
	}
}

type StrategyKey struct {
	ConcurrentIdx    uint32
	Keys             []string
	Env              string
	SubTaskScheduler *SubTaskScheduler
	OnFinishSubTask  func(*SubTask)
}

func (s *TaskScheduler) Schedule(ctx context.Context, builder *TaskBuilder) (*TaskGroup, error) {
	if s.step.Strategy == nil {
		task, err := builder.Build(ctx, &s.step)
		if err != nil {
			return nil, err
		}
		return NewTaskGroup([]*Task{task}), nil
	}
	strategy := s.step.Strategy
	keys, err := s.getScheduleKeys(ctx, builder, strategy.Key.Source)
	if err != nil {
		return nil, err
	}
	subTaskScheduler := NewSubTaskScheduler(strategy.Scheduler.MaxConcurrentNumPerPod)
	switch {
	case strategy.Scheduler.MaxPodNum != 0:
		return s.maxPodNumBasedSchedule(ctx, builder, keys, subTaskScheduler)
	case strategy.Scheduler.MaxContainersPerPod != 0:
		return s.maxContainersBasedSchedule(ctx, builder, keys, subTaskScheduler)
	}
	return nil, fmt.Errorf("kubetest: unsupecified scheduler parameter. maxPodNum or maxContainersPerPod must be specified")
}

func (s *TaskScheduler) maxContainersBasedSchedule(ctx context.Context, builder *TaskBuilder, keys []string, subTaskScheduler *SubTaskScheduler) (*TaskGroup, error) {
	strategy := s.step.Strategy
	maxContainers := uint32(strategy.Scheduler.MaxContainersPerPod)

	var (
		finishedKeyNum uint32
		finishedKeyMu  sync.Mutex
		keyNum         uint32 = uint32(len(keys))
	)
	if keyNum <= maxContainers {
		task, err := builder.BuildWithKey(ctx, &s.step, &StrategyKey{
			ConcurrentIdx:    0,
			Keys:             keys,
			SubTaskScheduler: subTaskScheduler,
			Env:              strategy.Key.Env,
			OnFinishSubTask: func(_ *SubTask) {
				finishedKeyMu.Lock()
				defer finishedKeyMu.Unlock()
				finishedKeyNum++
				LoggerFromContext(ctx).Info(
					"%d/%d (%f%%) finished.",
					finishedKeyNum, keyNum, (float32(finishedKeyNum)/float32(keyNum))*100,
				)
			},
		})
		if err != nil {
			return nil, err
		}
		return NewTaskGroup([]*Task{task}), nil
	}
	concurrent := keyNum / maxContainers
	tasks := []*Task{}
	sum := uint32(0)
	for i := uint32(0); i <= concurrent; i++ {
		var taskKeys []string
		if i == concurrent {
			taskKeys = keys[sum:]
		} else {
			taskKeys = keys[sum : sum+maxContainers]
		}
		taskNum := uint32(len(taskKeys))
		if taskNum == 0 {
			// if 'keyNum % maxContaienrs' is zero, taskKeys goes to zero in the last loop.
			continue
		}
		task, err := builder.BuildWithKey(ctx, &s.step, &StrategyKey{
			ConcurrentIdx:    i,
			Keys:             taskKeys,
			SubTaskScheduler: subTaskScheduler,
			Env:              strategy.Key.Env,
			OnFinishSubTask: func(_ *SubTask) {
				finishedKeyMu.Lock()
				defer finishedKeyMu.Unlock()
				finishedKeyNum++
				LoggerFromContext(ctx).Info(
					"%d/%d (%f%%) finished.",
					finishedKeyNum, keyNum, (float32(finishedKeyNum)/float32(keyNum))*100,
				)
			},
		})
		if err != nil {
			return nil, err
		}
		tasks = append(tasks, task)
		sum += taskNum
	}
	if keyNum != sum {
		return nil, fmt.Errorf("kubetest: failed to schedule: required key num %d but scheduled key num %d", keyNum, sum)
	}
	return NewTaskGroup(tasks), nil
}

func (s *TaskScheduler) maxPodNumBasedSchedule(ctx context.Context, builder *TaskBuilder, keys []string, subTaskScheduler *SubTaskScheduler) (*TaskGroup, error) {
	strategy := s.step.Strategy
	maxPods := uint32(strategy.Scheduler.MaxPodNum)

	var (
		finishedKeyNum uint32
		finishedKeyMu  sync.Mutex
		keyNum         uint32 = uint32(len(keys))
		tasks          []*Task
	)
	if keyNum < maxPods {
		// If there are more Pods in use than the number of keys, launch as many Pods as there are keys.
		for i := uint32(0); i < keyNum; i++ {
			task, err := builder.BuildWithKey(ctx, &s.step, &StrategyKey{
				ConcurrentIdx:    i,
				Keys:             []string{keys[i]},
				SubTaskScheduler: subTaskScheduler,
				Env:              strategy.Key.Env,
				OnFinishSubTask: func(_ *SubTask) {
					finishedKeyMu.Lock()
					defer finishedKeyMu.Unlock()
					finishedKeyNum++
					LoggerFromContext(ctx).Info(
						"%d/%d (%f%%) finished.",
						finishedKeyNum, keyNum, (float32(finishedKeyNum)/float32(keyNum))*100,
					)
				},
			})
			if err != nil {
				return nil, err
			}
			tasks = append(tasks, task)
		}
		return NewTaskGroup(tasks), nil
	}

	perPodKeyNum := keyNum / maxPods
	sum := uint32(0)
	for i := uint32(0); i < maxPods; i++ {
		var taskKeys []string
		if i == (maxPods - 1) {
			taskKeys = keys[sum:]
		} else {
			taskKeys = keys[sum : sum+perPodKeyNum]
		}
		taskNum := uint32(len(taskKeys))
		if taskNum == 0 {
			break
		}
		task, err := builder.BuildWithKey(ctx, &s.step, &StrategyKey{
			ConcurrentIdx:    i,
			Keys:             taskKeys,
			SubTaskScheduler: subTaskScheduler,
			Env:              strategy.Key.Env,
			OnFinishSubTask: func(_ *SubTask) {
				finishedKeyMu.Lock()
				defer finishedKeyMu.Unlock()
				finishedKeyNum++
				LoggerFromContext(ctx).Info(
					"%d/%d (%f%%) finished.",
					finishedKeyNum, keyNum, (float32(finishedKeyNum)/float32(keyNum))*100,
				)
			},
		})
		if err != nil {
			return nil, err
		}
		tasks = append(tasks, task)
		sum += taskNum
	}
	if keyNum != sum {
		return nil, fmt.Errorf("kubetest: failed to schedule: required key num %d but scheduled key num %d", keyNum, sum)
	}
	return NewTaskGroup(tasks), nil
}

func (s *TaskScheduler) getScheduleKeys(ctx context.Context, builder *TaskBuilder, source StrategyKeySource) ([]string, error) {
	switch {
	case len(source.Static) > 0:
		LoggerFromContext(ctx).Info(
			"found %d static keys to start distributed task",
			len(source.Static),
		)
		return source.Static, nil
	case source.Dynamic != nil:
		return s.dynamicKeys(ctx, builder, source.Dynamic)
	default:
		return nil, fmt.Errorf("kubetest: invalid schedule key source")
	}
}

func (s *TaskScheduler) dynamicKeys(ctx context.Context, builder *TaskBuilder, source *StrategyDynamicKeySource) ([]string, error) {
	LoggerFromContext(ctx).Info("start to get dynamic task keys for running distributed task")
	keyTask, err := builder.Build(ctx, &MainStep{Template: source.Template})
	if err != nil {
		return nil, err
	}
	result, err := keyTask.Run(ctx)
	if err != nil {
		return nil, err
	}
	mainResults := result.MainTaskResults()
	if len(mainResults) == 0 {
		return nil, fmt.Errorf("kubetest: failed to find main task results for dynamic keys")
	}
	if len(mainResults) > 1 {
		return nil, fmt.Errorf("kubetest: found multiple main task results")
	}
	if mainResults[0].Err != nil {
		return nil, fmt.Errorf("kubetest: failed to get dynamic key task: %w", mainResults[0].Err)
	}
	out := mainResults[0].Out
	filter, err := s.sourceFilter(source.Filter)
	if err != nil {
		return nil, err
	}
	keys := []string{}
	for _, key := range strings.Split(string(out), s.sourceDelim(source.Delim)) {
		if strings.TrimSpace(key) == "" {
			continue
		}
		if filter != nil && !filter.MatchString(key) {
			continue
		}
		keys = append(keys, key)
	}
	LoggerFromContext(ctx).Info("found %d dynamic keys to start distributed task", len(keys))
	return keys, nil
}

func (s *TaskScheduler) sourceFilter(filter string) (*regexp.Regexp, error) {
	if filter == "" {
		return nil, nil
	}
	return regexp.Compile(filter)
}

func (s *TaskScheduler) sourceDelim(delim string) string {
	const (
		defaultDelim = "\n"
	)
	if delim == "" {
		return defaultDelim
	}
	return delim
}

func NewSubTaskScheduler(maxConcurrentNumPerPod int) *SubTaskScheduler {
	return &SubTaskScheduler{
		maxConcurrentNumPerPod: maxConcurrentNumPerPod,
	}
}

type SubTaskScheduler struct {
	maxConcurrentNumPerPod int
}

func (s *SubTaskScheduler) Schedule(tasks []*SubTask) []*SubTaskGroup {
	concurrentNum := s.getConcurrentNum(len(tasks))
	taskNum := len(tasks)
	groups := []*SubTaskGroup{}
	if concurrentNum > 0 {
		concurrent := concurrentNum
		for i := 0; i < taskNum; i += concurrent {
			start := i
			end := i + concurrent
			if end > taskNum {
				end = taskNum
			}
			groups = append(groups, NewSubTaskGroup(tasks[start:end]))
		}
	} else {
		groups = append(groups, NewSubTaskGroup(tasks))
	}
	return groups
}

func (s *SubTaskScheduler) getConcurrentNum(taskNum int) int {
	maxConcurrentNum := s.maxConcurrentNumPerPod
	if maxConcurrentNum <= 0 {
		return taskNum
	}
	if maxConcurrentNum > taskNum {
		return taskNum
	}
	return maxConcurrentNum
}
