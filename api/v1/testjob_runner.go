// +build !ignore_autogenerated

package v1

import (
	"bytes"
	"context"
	"encoding/base64"
	"encoding/json"
	"fmt"
	"log"
	"os"
	"path/filepath"
	"regexp"
	"strings"
	"sync"
	"time"

	"github.com/goccy/kubejob"
	"github.com/rs/xid"
	"golang.org/x/sync/errgroup"
	"golang.org/x/xerrors"
	batchv1 "k8s.io/api/batch/v1"
	apiv1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime/schema"
	"k8s.io/client-go/kubernetes"
	"k8s.io/client-go/rest"
)

const (
	gitImageName         = "alpine/git"
	oauthTokenEnv        = "OAUTH_TOKEN"
	sharedVolumeName     = "repo"
	defaultListDelimiter = "\n"
)

var (
	ErrFailedTestJob = xerrors.New("failed test job")
)

type TestResult string

const (
	TestResultSuccess TestResult = "success"
	TestResultFailure TestResult = "failure"
)

type TestResultLog struct {
	TestResult     TestResult          `json:"testResult"`
	Job            string              `json:"job"`
	ElapsedTimeSec int                 `json:"elapsedTimeSec"`
	StartedAt      time.Time           `json:"startedAt"`
	Details        TestResultLogDetail `json:"details"`
}

type TestResultLogDetail struct {
	Tests []TestLog `json:"tests"`
}

type TestLog struct {
	Name           string     `json:"name"`
	TestResult     TestResult `json:"testResult"`
	ElapsedTimeSec int        `json:"elapsedTimeSec"`
	Message        string     `json:"-"`
}

type TestJobRunner struct {
	token              string
	disabledPrepareLog bool
	disabledCommandLog bool
	disabledResultLog  bool
	logger             func(*kubejob.ContainerLog)
	config             *rest.Config
	clientSet          *kubernetes.Clientset
	printMu            sync.Mutex
}

func NewTestJobRunner(config *rest.Config) (*TestJobRunner, error) {
	cs, err := kubernetes.NewForConfig(config)
	if err != nil {
		return nil, xerrors.Errorf("failed to create clientset: %w", err)
	}
	return &TestJobRunner{
		config:    config,
		clientSet: cs,
	}, nil
}

func (r *TestJobRunner) SetToken(token string) {
	r.token = token
}

func (r *TestJobRunner) sharedVolume() apiv1.Volume {
	return apiv1.Volume{
		Name: sharedVolumeName,
		VolumeSource: apiv1.VolumeSource{
			EmptyDir: &apiv1.EmptyDirVolumeSource{},
		},
	}
}

func (r *TestJobRunner) sharedVolumeMount(job TestJob) apiv1.VolumeMount {
	var mountPath string
	if job.Spec.Git.CheckoutDir != "" {
		mountPath = job.Spec.Git.CheckoutDir
	} else {
		mountPath = filepath.Join("/", "git", "workspace")
	}
	return apiv1.VolumeMount{
		Name:      sharedVolumeName,
		MountPath: mountPath,
	}
}

func (r *TestJobRunner) gitImage(job TestJob) string {
	if job.Spec.Git.Image != "" {
		return job.Spec.Git.Image
	}
	return gitImageName
}

func (r *TestJobRunner) cloneURL(job TestJob) string {
	repo := job.Spec.Git.Repo
	if r.token != "" {
		return fmt.Sprintf("https://$(%s)@%s.git", oauthTokenEnv, repo)
	}
	return fmt.Sprintf("https://%s.git", repo)
}

func (r *TestJobRunner) gitCloneContainer(job TestJob) apiv1.Container {
	cloneURL := r.cloneURL(job)
	cloneCmd := []string{"clone"}
	volumeMount := r.sharedVolumeMount(job)
	branch := job.Spec.Git.Branch
	if branch != "" {
		cloneCmd = append(cloneCmd, "-b", branch, cloneURL, volumeMount.MountPath)
	} else {
		cloneCmd = append(cloneCmd, cloneURL, volumeMount.MountPath)
	}
	return apiv1.Container{
		Name:         "kubetest-init-clone",
		Image:        r.gitImage(job),
		Command:      []string{"git"},
		Args:         cloneCmd,
		Env:          []apiv1.EnvVar{{Name: oauthTokenEnv, Value: r.token}},
		VolumeMounts: []apiv1.VolumeMount{volumeMount},
	}
}

func (r *TestJobRunner) gitSwitchContainer(job TestJob) apiv1.Container {
	volumeMount := r.sharedVolumeMount(job)
	return apiv1.Container{
		Name:         "kubetest-init-switch",
		Image:        r.gitImage(job),
		WorkingDir:   volumeMount.MountPath,
		Command:      []string{"git"},
		Args:         []string{"checkout", "--detach", job.Spec.Git.Rev},
		VolumeMounts: []apiv1.VolumeMount{volumeMount},
	}
}

func (r *TestJobRunner) gitConfigUserEmailContainer(job TestJob) apiv1.Container {
	volumeMount := r.sharedVolumeMount(job)
	return apiv1.Container{
		Name:         "kubetest-init-git-config-user-email",
		Image:        r.gitImage(job),
		WorkingDir:   volumeMount.MountPath,
		Command:      []string{"git"},
		Args:         []string{"config", "user.email", "anonymous@kubetest.com"},
		VolumeMounts: []apiv1.VolumeMount{volumeMount},
	}
}

func (r *TestJobRunner) gitConfigUserNameContainer(job TestJob) apiv1.Container {
	volumeMount := r.sharedVolumeMount(job)
	return apiv1.Container{
		Name:         "kubetest-init-git-config-user-name",
		Image:        r.gitImage(job),
		WorkingDir:   volumeMount.MountPath,
		Command:      []string{"git"},
		Args:         []string{"config", "user.name", "anonymous"},
		VolumeMounts: []apiv1.VolumeMount{volumeMount},
	}
}

func (r *TestJobRunner) gitMergeContainer(job TestJob) apiv1.Container {
	volumeMount := r.sharedVolumeMount(job)
	return apiv1.Container{
		Name:         "kubetest-init-merge",
		Image:        r.gitImage(job),
		WorkingDir:   volumeMount.MountPath,
		Command:      []string{"git"},
		Args:         []string{"pull", "origin", job.Spec.Git.Merge.Base},
		VolumeMounts: []apiv1.VolumeMount{volumeMount},
	}
}

func (r *TestJobRunner) initContainers(job TestJob) []apiv1.Container {
	containers := []apiv1.Container{}
	if job.Spec.Git.Branch != "" {
		containers = append(containers, r.gitCloneContainer(job))
	} else {
		containers = append(containers, r.gitCloneContainer(job), r.gitSwitchContainer(job))
	}
	if job.Spec.Git.Merge.Base != "" {
		containers = append(containers,
			r.gitConfigUserEmailContainer(job),
			r.gitConfigUserNameContainer(job),
			r.gitMergeContainer(job),
		)
	}
	return containers
}

func (r *TestJobRunner) command(cmd Command) ([]string, []string) {
	e := base64.StdEncoding.EncodeToString([]byte(string(cmd)))
	return []string{"sh"}, []string{"-c", fmt.Sprintf("echo %s | base64 -d | sh", e)}
}

func (r *TestJobRunner) commandText(cmd Command) string {
	c, args := r.command(cmd)
	return strings.Join(append(c, args...), " ")
}

func (r *TestJobRunner) DisablePrepareLog() {
	r.disabledPrepareLog = true
}

func (r *TestJobRunner) DisableCommandLog() {
	r.disabledCommandLog = true
}

func (r *TestJobRunner) DisableResultLog() {
	r.disabledResultLog = true
}

func (r *TestJobRunner) SetLogger(logger func(*kubejob.ContainerLog)) {
	r.logger = logger
}

func (r *TestJobRunner) Run(ctx context.Context, testjob TestJob) error {
	testLog := TestResultLog{Job: testjob.ObjectMeta.Name, StartedAt: time.Now()}

	defer func(start time.Time) {
		if r.disabledResultLog {
			return
		}
		testLog.ElapsedTimeSec = int(time.Since(start).Seconds())
		b, _ := json.Marshal(testLog)

		var logMap map[string]interface{}
		json.Unmarshal(b, &logMap)

		for k, v := range testjob.Spec.Log.ExtParam {
			logMap[k] = v
		}
		b, _ = json.Marshal(logMap)
		fmt.Println(string(b))
	}(time.Now())

	testLogs, err := r.run(ctx, testjob)
	testLog.Details = TestResultLogDetail{
		Tests: testLogs,
	}
	if err != nil {
		testLog.TestResult = TestResultFailure
		return err
	}
	testLog.TestResult = TestResultSuccess
	return nil
}

func (r *TestJobRunner) run(ctx context.Context, testjob TestJob) ([]TestLog, error) {
	if testjob.Spec.Git.Branch == "" && testjob.Spec.Git.Rev == "" {
		testjob.Spec.Git.Branch = "master"
	}
	token := testjob.Spec.Git.Token
	if token != nil {
		secret, err := r.clientSet.CoreV1().
			Secrets(testjob.Namespace).
			Get(token.SecretKeyRef.Name, metav1.GetOptions{})
		if err != nil {
			return nil, err
		}
		data, exists := secret.Data[token.SecretKeyRef.Key]
		if !exists {
			gr := schema.GroupResource{
				Group:    GroupVersion.Group,
				Resource: "TestJob",
			}
			return nil, errors.NewNotFound(gr, "token")
		}
		r.token = strings.TrimSpace(string(data))
	}
	if err := r.prepare(ctx, testjob); err != nil {
		return nil, err
	}
	if testjob.Spec.DistributedTest != nil {
		return r.runDistributedTest(ctx, testjob)
	}
	return r.runTest(ctx, testjob)
}

func (r *TestJobRunner) prepareImage(stepIdx int, testjob TestJob) string {
	step := testjob.Spec.Prepare.Steps[stepIdx]
	if step.Image != "" {
		return step.Image
	}
	return testjob.Spec.Prepare.Image
}

func (r *TestJobRunner) prepareWorkingDir(stepIdx int, testjob TestJob) string {
	step := testjob.Spec.Prepare.Steps[stepIdx]
	if step.Workdir != "" {
		return step.Workdir
	}
	return r.sharedVolumeMount(testjob).MountPath
}

func (r *TestJobRunner) prepareEnv(stepIdx int, testjob TestJob) []apiv1.EnvVar {
	return testjob.Spec.Prepare.Steps[stepIdx].Env
}

func (r *TestJobRunner) enabledPrepareCheckout(testjob TestJob) bool {
	checkout := testjob.Spec.Prepare.Checkout
	if checkout != nil && !(*checkout) {
		return false
	}
	return true
}

func (r *TestJobRunner) enabledCheckout(testjob TestJob) bool {
	checkout := testjob.Spec.Git.Checkout
	if checkout != nil && !(*checkout) {
		return false
	}
	return true
}

func (r *TestJobRunner) generateName(name string) string {
	return fmt.Sprintf("%s-%s", name, xid.New())
}

func (r *TestJobRunner) prepare(ctx context.Context, testjob TestJob) error {
	if len(testjob.Spec.Prepare.Steps) == 0 {
		return nil
	}
	var containers []apiv1.Container
	if r.enabledPrepareCheckout(testjob) {
		containers = r.initContainers(testjob)
	}
	fmt.Println("run prepare")
	startPrepareTime := time.Now()
	defer func() {
		fmt.Fprintf(os.Stderr, "prepare: elapsed time %f sec\n", time.Since(startPrepareTime).Seconds())
	}()
	for stepIdx, step := range testjob.Spec.Prepare.Steps {
		image := r.prepareImage(stepIdx, testjob)
		cmd, args := r.command(step.Command)
		volumeMount := r.sharedVolumeMount(testjob)
		containers = append(containers, apiv1.Container{
			Name:       step.Name,
			Image:      image,
			Command:    cmd,
			Args:       args,
			WorkingDir: r.prepareWorkingDir(stepIdx, testjob),
			VolumeMounts: []apiv1.VolumeMount{
				volumeMount,
			},
			Env: r.prepareEnv(stepIdx, testjob),
		})
	}
	lastContainer := containers[len(containers)-1]
	initContainers := []apiv1.Container{}
	if len(containers) > 1 {
		initContainers = containers[:len(containers)-1]
	}
	job, err := kubejob.NewJobBuilder(r.config, testjob.Namespace).
		BuildWithJob(&batchv1.Job{
			ObjectMeta: metav1.ObjectMeta{
				Name: r.generateName(testjob.ObjectMeta.Name),
			},
			Spec: batchv1.JobSpec{
				Template: apiv1.PodTemplateSpec{
					Spec: apiv1.PodSpec{
						Volumes: []apiv1.Volume{
							r.sharedVolume(),
						},
						InitContainers:   initContainers,
						Containers:       []apiv1.Container{lastContainer},
						ImagePullSecrets: testjob.Spec.Template.Spec.ImagePullSecrets,
					},
				},
			},
		})
	if err != nil {
		return err
	}
	job.DisableCommandLog()
	if r.logger != nil {
		job.SetLogger(r.logger)
	}
	return job.Run(ctx)
}

func (r *TestJobRunner) newJobForTesting(testjob TestJob, containers ...apiv1.Container) (*kubejob.Job, error) {
	var initContainers []apiv1.Container
	if r.enabledCheckout(testjob) {
		initContainers = r.initContainers(testjob)
	}
	template := testjob.Spec.Template
	template.Spec.InitContainers = append(initContainers, template.Spec.InitContainers...)
	testContainers := []apiv1.Container{}
	var testContainerName string
	if testjob.Spec.DistributedTest != nil {
		testContainerName = testjob.Spec.DistributedTest.ContainerName
	}
	for _, container := range template.Spec.Containers {
		if len(containers) > 0 && container.Name == testContainerName {
			// skip default test container
			continue
		}
		container.VolumeMounts = append(container.VolumeMounts, r.sharedVolumeMount(testjob))
		testContainers = append(testContainers, container)
	}
	for _, container := range containers {
		container.VolumeMounts = append(container.VolumeMounts, r.sharedVolumeMount(testjob))
		testContainers = append(testContainers, container)
	}
	template.Spec.Containers = testContainers
	template.Spec.Volumes = append(template.Spec.Volumes, r.sharedVolume())
	return kubejob.NewJobBuilder(r.config, testjob.Namespace).
		BuildWithJob(&batchv1.Job{
			ObjectMeta: metav1.ObjectMeta{
				Name: r.generateName(testjob.ObjectMeta.Name),
			},
			Spec: batchv1.JobSpec{
				Template: template,
			},
		})
}

func (r *TestJobRunner) runTest(ctx context.Context, testjob TestJob) ([]TestLog, error) {
	job, err := r.newJobForTesting(testjob)
	if err != nil {
		return nil, err
	}
	if r.logger != nil {
		job.SetLogger(r.logger)
	}
	if r.disabledPrepareLog {
		job.DisableInitContainerLog()
	}
	if r.disabledCommandLog {
		job.DisableCommandLog()
	}
	if err := job.Run(ctx); err != nil {
		var failedJob *kubejob.FailedJob
		if xerrors.As(err, &failedJob) {
			return nil, ErrFailedTestJob
		}
		log.Printf(err.Error())
		return nil, ErrFailedTestJob
	}
	return nil, nil
}

func (r *TestJobRunner) testContainer(testjob TestJob) (*apiv1.Container, error) {
	testContainerName := testjob.Spec.DistributedTest.ContainerName
	for _, container := range testjob.Spec.Template.Spec.Containers {
		if container.Name == testContainerName {
			return &container, nil
		}
	}
	return nil, xerrors.Errorf("cannot find container for running test by name")
}

func (r *TestJobRunner) commandString(c *apiv1.Container) string {
	s := []string{}
	s = append(s, c.Command...)
	s = append(s, c.Args...)
	return strings.Join(s, " ")
}

func (r *TestJobRunner) runDistributedTest(ctx context.Context, testjob TestJob) ([]TestLog, error) {
	testContainer, err := r.testContainer(testjob)
	if err != nil {
		return nil, xerrors.Errorf("failed to find test container: %w", err)
	}
	fmt.Println("get listing of tests...")
	list, err := r.testList(ctx, testjob)
	if err != nil {
		return nil, xerrors.Errorf("failed to get list for testing: %w", err)
	}
	if len(list) == 0 {
		return nil, nil
	}
	plan := r.plan(testjob, list)

	defer func(start time.Time) {
		fmt.Fprintf(os.Stderr, "test: elapsed time %f sec\n", time.Since(start).Seconds())
	}(time.Now())

	testLogs := []TestLog{}
	testLogMu := sync.Mutex{}

	var eg errgroup.Group
	for podIdx, tests := range plan {
		podIdx := podIdx
		tests := tests
		eg.Go(func() error {
			logs, err := r.runTests(ctx, testjob, testContainer, podIdx, tests)
			if err != nil {
				return xerrors.Errorf("failed to runTests: %w", err)
			}
			testLogMu.Lock()
			testLogs = append(testLogs, logs...)
			testLogMu.Unlock()
			return nil
		})
	}
	if err := eg.Wait(); err != nil {
		return nil, xerrors.Errorf("failed to distributed test job: %w", err)
	}

	failedTestLogs := []TestLog{}
	for _, testLog := range testLogs {
		if testLog.TestResult == TestResultFailure {
			failedTestLogs = append(failedTestLogs, testLog)
		}
	}
	if len(failedTestLogs) > 0 {
		if !testjob.Spec.DistributedTest.Retest.Enabled {
			return testLogs, ErrFailedTestJob
		}
		fmt.Println("start retest....")
		tests := []string{}
		for _, log := range failedTestLogs {
			tests = append(tests, log.Name)
		}
		if _, err := r.runTests(ctx, testjob, testContainer, 0, tests); err != nil {
			return testLogs, ErrFailedTestJob
		}
	}
	return testLogs, nil
}

type command struct {
	cmd       []string
	args      []string
	test      string
	container string
	startedAt time.Time
}

type commands []*command

func (c commands) commandValueMap() map[string]*command {
	m := map[string]*command{}
	for _, cc := range c {
		m[cc.test] = cc
	}
	return m
}

func (r *TestJobRunner) testCommand(cmd []string, args []string, test string) *command {
	return &command{
		cmd:  cmd,
		args: args,
		test: test,
	}
}

func (r *TestJobRunner) testsToCommands(c *apiv1.Container, tests []string) commands {
	commands := []*command{}
	for _, test := range tests {
		cmd := r.testCommand(c.Command, c.Args, test)
		commands = append(commands, cmd)
	}
	return commands
}

func (r *TestJobRunner) testContainerWorkingDir(testContainer *apiv1.Container, testjob TestJob) string {
	workingDir := testContainer.WorkingDir
	if workingDir == "" {
		return r.sharedVolumeMount(testjob).MountPath
	}
	return workingDir
}

func (r *TestJobRunner) printTestLog(idx int, log string) {
	r.printMu.Lock()
	defer r.printMu.Unlock()
	for _, line := range strings.Split(log, "\n") {
		fmt.Fprintf(os.Stderr, "[POD %d] %s\n", idx, line)
	}
}

func (r *TestJobRunner) runTests(ctx context.Context, testjob TestJob, testContainer *apiv1.Container, podIdx int, tests []string) ([]TestLog, error) {
	testCommands := r.testsToCommands(testContainer, tests)

	testContainerWorkingDir := r.testContainerWorkingDir(testContainer, testjob)
	testContainer.WorkingDir = testContainerWorkingDir

	containers := []apiv1.Container{}
	for _, test := range tests {
		container := testContainer.DeepCopy()
		container.Name = ""
		container.Env = append(container.Env, apiv1.EnvVar{
			Name:  "TEST",
			Value: test,
		})
		containers = append(containers, *container)
	}
	job, err := r.newJobForTesting(testjob, containers...)
	if err != nil {
		return nil, err
	}
	for _, cache := range testjob.Spec.DistributedTest.Cache {
		cmd, args := r.command(cache.Command)
		volumeMounts := append(testContainer.VolumeMounts, r.sharedVolumeMount(testjob), apiv1.VolumeMount{
			Name:      cache.Name,
			MountPath: cache.Path,
		})
		cacheContainer := apiv1.Container{
			Name:         cache.Name,
			Image:        testContainer.Image,
			Command:      cmd,
			Args:         args,
			WorkingDir:   testContainerWorkingDir,
			VolumeMounts: volumeMounts,
			Env:          testContainer.Env,
		}
		job.Spec.Template.Spec.Volumes = append(job.Spec.Template.Spec.Volumes, apiv1.Volume{
			Name: cache.Name,
			VolumeSource: apiv1.VolumeSource{
				EmptyDir: &apiv1.EmptyDirVolumeSource{},
			},
		})
		job.Spec.Template.Spec.InitContainers = append(job.Spec.Template.Spec.InitContainers, cacheContainer)
	}
	for i := 0; i < len(testCommands); i++ {
		containerName := job.Spec.Template.Spec.Containers[i].Name
		testCommands[i].container = containerName
	}
	job.DisableCommandLog()
	testLogs := []TestLog{}
	var failedJob *kubejob.FailedJob
	if err := job.RunWithExecutionHandler(ctx, func(executors []*kubejob.JobExecutor) error {
		testExecutors := []*kubejob.JobExecutor{}
		sidecarExecutors := []*kubejob.JobExecutor{}
		for _, executor := range executors {
			isTestContainer := false
			for _, env := range executor.Container.Env {
				if env.Name == "TEST" {
					isTestContainer = true
					break
				}
			}
			if isTestContainer {
				testExecutors = append(testExecutors, executor)
			} else {
				sidecarExecutors = append(sidecarExecutors, executor)
			}
		}
		for _, sidecar := range sidecarExecutors {
			sidecar := sidecar
			go func() {
				sidecar.Exec()
			}()
		}
		concurrent := testjob.Spec.DistributedTest.MaxConcurrentNumPerPod
		testExecutorNum := len(testExecutors)
		if concurrent <= 0 {
			concurrent = testExecutorNum
		} else if concurrent > testExecutorNum {
			concurrent = testExecutorNum
		}
		for i := 0; i < testExecutorNum; i += concurrent {
			start := i
			end := i + concurrent
			if end > testExecutorNum {
				end = testExecutorNum
			}
			executors := testExecutors[start:end]
			var (
				eg errgroup.Group
				mu sync.Mutex
			)
			for _, executor := range executors {
				executor := executor
				eg.Go(func() error {
					start := time.Now()
					out, err := executor.Exec()
					elapsedTime := int(time.Since(start).Seconds())
					mu.Lock()
					defer mu.Unlock()
					var testName string
					for _, env := range executor.Container.Env {
						if env.Name == "TEST" {
							testName = env.Value
							break
						}
					}
					testResult := TestResultSuccess
					if err != nil {
						testResult = TestResultFailure
					}
					testLogs = append(testLogs, TestLog{
						Name:           testName,
						TestResult:     testResult,
						ElapsedTimeSec: elapsedTime,
						Message:        string(out),
					})
					r.printTestLog(
						podIdx,
						fmt.Sprintf("TEST=%s; %s\n%s", testName, r.commandString(testContainer), string(out)),
					)
					return nil
				})
			}
			eg.Wait()
		}
		return nil
	}); err != nil {
		if !xerrors.As(err, &failedJob) {
			return nil, err
		}
	}
	return testLogs, nil
}

func (r *TestJobRunner) testList(ctx context.Context, testjob TestJob) ([]string, error) {
	startListTime := time.Now()
	defer func() {
		fmt.Fprintf(os.Stderr, "list: elapsed time %f sec\n", time.Since(startListTime).Seconds())
	}()
	distributedTest := testjob.Spec.DistributedTest

	listjob := testjob
	container, err := r.testContainer(listjob)
	if err != nil {
		return nil, xerrors.Errorf("failed to find container for list: %w", err)
	}
	container.WorkingDir = r.testContainerWorkingDir(container, listjob)
	container.Command = distributedTest.List.Command
	container.Args = distributedTest.List.Args
	listjob.Spec.Template.Spec.Containers = []apiv1.Container{*container}
	listjob.Spec.Prepare.Steps = []PrepareStepSpec{}
	listjob.Spec.DistributedTest = nil

	listJobRunner, err := NewTestJobRunner(r.config)
	if err != nil {
		return nil, xerrors.Errorf("failed to create test job runner: %w", err)
	}
	listJobRunner.DisablePrepareLog()
	listJobRunner.DisableCommandLog()
	listJobRunner.DisableResultLog()

	var pattern *regexp.Regexp
	if distributedTest.List.Pattern != "" {
		reg, err := regexp.Compile(distributedTest.List.Pattern)
		if err != nil {
			return nil, xerrors.Errorf("failed to compile pattern for distributed testing: %w", err)
		}
		pattern = reg
	}

	var b bytes.Buffer
	listJobRunner.SetLogger(func(log *kubejob.ContainerLog) {
		b.WriteString(log.Log)
	})
	if err := listJobRunner.Run(ctx, listjob); err != nil {
		listJobRunner.disabledPrepareLog = false
		listJobRunner.disabledCommandLog = false
		b.Reset()
		listJobRunner.Run(ctx, listjob)
		return nil, xerrors.Errorf("failed to run listJob %s: %w", b.String(), err)
	}
	delim := distributedTest.List.Delimiter
	if delim == "" {
		delim = "\n"
	}
	tests := []string{}
	result := b.String()
	list := strings.Split(result, delim)
	if pattern != nil {
		for _, name := range list {
			if pattern.MatchString(name) {
				tests = append(tests, name)
			}
		}
	} else {
		tests = list
	}
	return tests, nil
}

func (r *TestJobRunner) plan(job TestJob, list []string) [][]string {
	maxContainers := job.Spec.DistributedTest.MaxContainersPerPod

	if len(list) <= maxContainers {
		return [][]string{list}
	}
	concurrent := len(list) / maxContainers
	plan := [][]string{}
	sum := 0
	for i := 0; i <= concurrent; i++ {
		if i == concurrent {
			plan = append(plan, list[sum:])
		} else {
			plan = append(plan, list[sum:sum+maxContainers])
		}
		sum += maxContainers
	}
	return plan
}
