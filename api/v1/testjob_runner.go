// +build !ignore_autogenerated

package v1

import (
	"bytes"
	"context"
	"encoding/base64"
	"encoding/json"
	"fmt"
	"log"
	"os"
	"path/filepath"
	"regexp"
	"strings"
	"sync"
	"time"

	"github.com/goccy/kubejob"
	"github.com/rs/xid"
	"golang.org/x/sync/errgroup"
	"golang.org/x/xerrors"
	batchv1 "k8s.io/api/batch/v1"
	apiv1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime/schema"
	"k8s.io/client-go/kubernetes"
)

const (
	gitImageName         = "alpine/git"
	oauthTokenEnv        = "OAUTH_TOKEN"
	sharedVolumeName     = "repo"
	defaultListDelimiter = "\n"
)

var (
	ErrFailedTestJob = xerrors.New("failed test job")
)

type TestResult string

const (
	TestResultSuccess TestResult = "success"
	TestResultFailure TestResult = "failure"
)

type TestResultLog struct {
	TestResult     TestResult          `json:"testResult"`
	Job            string              `json:"job"`
	ElapsedTimeSec int                 `json:"elapsedTimeSec"`
	StartedAt      time.Time           `json:"startedAt"`
	Details        TestResultLogDetail `json:"details"`
}

type TestResultLogDetail struct {
	Tests []TestLog `json:"tests"`
}

type TestLog struct {
	Name           string     `json:"name"`
	TestResult     TestResult `json:"testResult"`
	ElapsedTimeSec int        `json:"elapsedTimeSec"`
	Message        string     `json:"-"`
}

type TestJobRunner struct {
	*kubernetes.Clientset
	token                     string
	disabledPrepareLog        bool
	disabledCommandLog        bool
	disabledResultLog         bool
	logger                    func(*kubejob.ContainerLog)
	containerNameToCommandMap sync.Map
}

func NewTestJobRunner(clientset *kubernetes.Clientset) *TestJobRunner {
	return &TestJobRunner{
		Clientset: clientset,
	}
}

func (r *TestJobRunner) SetToken(token string) {
	r.token = token
}

func (r *TestJobRunner) sharedVolume() apiv1.Volume {
	return apiv1.Volume{
		Name: sharedVolumeName,
		VolumeSource: apiv1.VolumeSource{
			EmptyDir: &apiv1.EmptyDirVolumeSource{},
		},
	}
}

func (r *TestJobRunner) sharedVolumeMount(job TestJob) apiv1.VolumeMount {
	var mountPath string
	if job.Spec.Git.CheckoutDir != "" {
		mountPath = job.Spec.Git.CheckoutDir
	} else {
		mountPath = filepath.Join("/", "git", "workspace")
	}
	return apiv1.VolumeMount{
		Name:      sharedVolumeName,
		MountPath: mountPath,
	}
}

func (r *TestJobRunner) gitImage(job TestJob) string {
	if job.Spec.Git.Image != "" {
		return job.Spec.Git.Image
	}
	return gitImageName
}

func (r *TestJobRunner) cloneURL(job TestJob) string {
	repo := job.Spec.Git.Repo
	if r.token != "" {
		return fmt.Sprintf("https://$(%s)@%s.git", oauthTokenEnv, repo)
	}
	return fmt.Sprintf("https://%s.git", repo)
}

func (r *TestJobRunner) gitCloneContainer(job TestJob) apiv1.Container {
	cloneURL := r.cloneURL(job)
	cloneCmd := []string{"clone"}
	volumeMount := r.sharedVolumeMount(job)
	branch := job.Spec.Git.Branch
	if branch != "" {
		cloneCmd = append(cloneCmd, "-b", branch, cloneURL, volumeMount.MountPath)
	} else {
		cloneCmd = append(cloneCmd, cloneURL, volumeMount.MountPath)
	}
	return apiv1.Container{
		Name:         "kubetest-init-clone",
		Image:        r.gitImage(job),
		Command:      []string{"git"},
		Args:         cloneCmd,
		Env:          []apiv1.EnvVar{{Name: oauthTokenEnv, Value: r.token}},
		VolumeMounts: []apiv1.VolumeMount{volumeMount},
	}
}

func (r *TestJobRunner) gitSwitchContainer(job TestJob) apiv1.Container {
	volumeMount := r.sharedVolumeMount(job)
	return apiv1.Container{
		Name:         "kubetest-init-switch",
		Image:        r.gitImage(job),
		WorkingDir:   volumeMount.MountPath,
		Command:      []string{"git"},
		Args:         []string{"checkout", "--detach", job.Spec.Git.Rev},
		VolumeMounts: []apiv1.VolumeMount{volumeMount},
	}
}

func (r *TestJobRunner) gitConfigUserEmailContainer(job TestJob) apiv1.Container {
	volumeMount := r.sharedVolumeMount(job)
	return apiv1.Container{
		Name:         "kubetest-init-git-config-user-email",
		Image:        r.gitImage(job),
		WorkingDir:   volumeMount.MountPath,
		Command:      []string{"git"},
		Args:         []string{"config", "user.email", "anonymous@kubetest.com"},
		VolumeMounts: []apiv1.VolumeMount{volumeMount},
	}
}

func (r *TestJobRunner) gitConfigUserNameContainer(job TestJob) apiv1.Container {
	volumeMount := r.sharedVolumeMount(job)
	return apiv1.Container{
		Name:         "kubetest-init-git-config-user-name",
		Image:        r.gitImage(job),
		WorkingDir:   volumeMount.MountPath,
		Command:      []string{"git"},
		Args:         []string{"config", "user.name", "anonymous"},
		VolumeMounts: []apiv1.VolumeMount{volumeMount},
	}
}

func (r *TestJobRunner) gitMergeContainer(job TestJob) apiv1.Container {
	volumeMount := r.sharedVolumeMount(job)
	return apiv1.Container{
		Name:         "kubetest-init-merge",
		Image:        r.gitImage(job),
		WorkingDir:   volumeMount.MountPath,
		Command:      []string{"git"},
		Args:         []string{"pull", "origin", job.Spec.Git.Merge.Base},
		VolumeMounts: []apiv1.VolumeMount{volumeMount},
	}
}

func (r *TestJobRunner) initContainers(job TestJob) []apiv1.Container {
	containers := []apiv1.Container{}
	if job.Spec.Git.Branch != "" {
		containers = append(containers, r.gitCloneContainer(job))
	} else {
		containers = append(containers, r.gitCloneContainer(job), r.gitSwitchContainer(job))
	}
	if job.Spec.Git.Merge.Base != "" {
		containers = append(containers,
			r.gitConfigUserEmailContainer(job),
			r.gitConfigUserNameContainer(job),
			r.gitMergeContainer(job),
		)
	}
	return containers
}

func (r *TestJobRunner) command(cmd Command) ([]string, []string) {
	e := base64.StdEncoding.EncodeToString([]byte(string(cmd)))
	return []string{"sh"}, []string{"-c", fmt.Sprintf("echo %s | base64 -d | sh", e)}
}

func (r *TestJobRunner) commandText(cmd Command) string {
	c, args := r.command(cmd)
	return strings.Join(append(c, args...), " ")
}

func (r *TestJobRunner) DisablePrepareLog() {
	r.disabledPrepareLog = true
}

func (r *TestJobRunner) DisableCommandLog() {
	r.disabledCommandLog = true
}

func (r *TestJobRunner) DisableResultLog() {
	r.disabledResultLog = true
}

func (r *TestJobRunner) SetLogger(logger func(*kubejob.ContainerLog)) {
	r.logger = logger
}

func (r *TestJobRunner) Run(ctx context.Context, testjob TestJob) error {
	testLog := TestResultLog{Job: testjob.ObjectMeta.Name, StartedAt: time.Now()}

	defer func(start time.Time) {
		if r.disabledResultLog {
			return
		}
		testLog.ElapsedTimeSec = int(time.Since(start).Seconds())
		b, _ := json.Marshal(testLog)

		var logMap map[string]interface{}
		json.Unmarshal(b, &logMap)

		for k, v := range testjob.Spec.Log.ExtParam {
			logMap[k] = v
		}
		b, _ = json.Marshal(logMap)
		fmt.Println(string(b))
	}(time.Now())

	testLogs, err := r.run(ctx, testjob)
	testLog.Details = TestResultLogDetail{
		Tests: testLogs,
	}
	if err != nil {
		testLog.TestResult = TestResultFailure
		return err
	}
	testLog.TestResult = TestResultSuccess
	return nil
}

func (r *TestJobRunner) run(ctx context.Context, testjob TestJob) ([]TestLog, error) {
	if testjob.Spec.Git.Branch == "" && testjob.Spec.Git.Rev == "" {
		testjob.Spec.Git.Branch = "master"
	}
	token := testjob.Spec.Git.Token
	if token != nil {
		secret, err := r.CoreV1().
			Secrets(testjob.Namespace).
			Get(token.SecretKeyRef.Name, metav1.GetOptions{})
		if err != nil {
			return nil, err
		}
		data, exists := secret.Data[token.SecretKeyRef.Key]
		if !exists {
			gr := schema.GroupResource{
				Group:    GroupVersion.Group,
				Resource: "TestJob",
			}
			return nil, errors.NewNotFound(gr, "token")
		}
		r.token = strings.TrimSpace(string(data))
	}
	if err := r.prepare(ctx, testjob); err != nil {
		return nil, err
	}
	if testjob.Spec.DistributedTest != nil {
		return r.runDistributedTest(ctx, testjob)
	}
	return r.runTest(ctx, testjob)
}

func (r *TestJobRunner) prepareImage(stepIdx int, testjob TestJob) string {
	step := testjob.Spec.Prepare.Steps[stepIdx]
	if step.Image != "" {
		return step.Image
	}
	return testjob.Spec.Prepare.Image
}

func (r *TestJobRunner) prepareWorkingDir(stepIdx int, testjob TestJob) string {
	step := testjob.Spec.Prepare.Steps[stepIdx]
	if step.Workdir != "" {
		return step.Workdir
	}
	return r.sharedVolumeMount(testjob).MountPath
}

func (r *TestJobRunner) prepareEnv(stepIdx int, testjob TestJob) []apiv1.EnvVar {
	return testjob.Spec.Prepare.Steps[stepIdx].Env
}

func (r *TestJobRunner) enabledPrepareCheckout(testjob TestJob) bool {
	checkout := testjob.Spec.Prepare.Checkout
	if checkout != nil && !(*checkout) {
		return false
	}
	return true
}

func (r *TestJobRunner) enabledCheckout(testjob TestJob) bool {
	checkout := testjob.Spec.Git.Checkout
	if checkout != nil && !(*checkout) {
		return false
	}
	return true
}

func (r *TestJobRunner) generateName(name string) string {
	return fmt.Sprintf("%s-%s", name, xid.New())
}

func (r *TestJobRunner) prepare(ctx context.Context, testjob TestJob) error {
	if len(testjob.Spec.Prepare.Steps) == 0 {
		return nil
	}
	var containers []apiv1.Container
	if r.enabledPrepareCheckout(testjob) {
		containers = r.initContainers(testjob)
	}
	fmt.Println("run prepare")
	startPrepareTime := time.Now()
	defer func() {
		fmt.Fprintf(os.Stderr, "prepare: elapsed time %f sec\n", time.Since(startPrepareTime).Seconds())
	}()
	for stepIdx, step := range testjob.Spec.Prepare.Steps {
		image := r.prepareImage(stepIdx, testjob)
		cmd, args := r.command(step.Command)
		volumeMount := r.sharedVolumeMount(testjob)
		containers = append(containers, apiv1.Container{
			Name:       step.Name,
			Image:      image,
			Command:    cmd,
			Args:       args,
			WorkingDir: r.prepareWorkingDir(stepIdx, testjob),
			VolumeMounts: []apiv1.VolumeMount{
				volumeMount,
			},
			Env: r.prepareEnv(stepIdx, testjob),
		})
	}
	lastContainer := containers[len(containers)-1]
	initContainers := []apiv1.Container{}
	if len(containers) > 1 {
		initContainers = containers[:len(containers)-1]
	}
	job, err := kubejob.NewJobBuilder(r.Clientset, testjob.Namespace).
		BuildWithJob(&batchv1.Job{
			ObjectMeta: metav1.ObjectMeta{
				Name: r.generateName(testjob.ObjectMeta.Name),
			},
			Spec: batchv1.JobSpec{
				Template: apiv1.PodTemplateSpec{
					Spec: apiv1.PodSpec{
						Volumes: []apiv1.Volume{
							r.sharedVolume(),
						},
						InitContainers:   initContainers,
						Containers:       []apiv1.Container{lastContainer},
						ImagePullSecrets: testjob.Spec.Template.Spec.ImagePullSecrets,
					},
				},
			},
		})
	if err != nil {
		return err
	}
	job.DisableCommandLog()
	if r.logger != nil {
		job.SetLogger(r.logger)
	}
	return job.Run(ctx)
}

func (r *TestJobRunner) newJobForTesting(testjob TestJob, containers ...apiv1.Container) (*kubejob.Job, error) {
	var initContainers []apiv1.Container
	if r.enabledCheckout(testjob) {
		initContainers = r.initContainers(testjob)
	}
	template := testjob.Spec.Template
	template.Spec.InitContainers = append(initContainers, template.Spec.InitContainers...)
	testContainers := []apiv1.Container{}
	var testContainerName string
	if testjob.Spec.DistributedTest != nil {
		testContainerName = testjob.Spec.DistributedTest.ContainerName
	}
	for _, container := range template.Spec.Containers {
		if len(containers) > 0 && container.Name == testContainerName {
			// skip default test container
			continue
		}
		container.VolumeMounts = append(container.VolumeMounts, r.sharedVolumeMount(testjob))
		testContainers = append(testContainers, container)
	}
	for _, container := range containers {
		container.VolumeMounts = append(container.VolumeMounts, r.sharedVolumeMount(testjob))
		testContainers = append(testContainers, container)
	}
	template.Spec.Containers = testContainers
	template.Spec.Volumes = append(template.Spec.Volumes, r.sharedVolume())
	return kubejob.NewJobBuilder(r.Clientset, testjob.Namespace).
		BuildWithJob(&batchv1.Job{
			ObjectMeta: metav1.ObjectMeta{
				Name: r.generateName(testjob.ObjectMeta.Name),
			},
			Spec: batchv1.JobSpec{
				Template: template,
			},
		})
}

func (r *TestJobRunner) runTest(ctx context.Context, testjob TestJob) ([]TestLog, error) {
	job, err := r.newJobForTesting(testjob)
	if err != nil {
		return nil, err
	}
	if r.logger != nil {
		job.SetLogger(r.logger)
	}
	if r.disabledPrepareLog {
		job.DisableInitContainerLog()
	}
	if r.disabledCommandLog {
		job.DisableCommandLog()
	}
	if err := job.Run(ctx); err != nil {
		var failedJob *kubejob.FailedJob
		if xerrors.As(err, &failedJob) {
			return nil, ErrFailedTestJob
		}
		log.Printf(err.Error())
		return nil, ErrFailedTestJob
	}
	return nil, nil
}

func (r *TestJobRunner) testContainer(testjob TestJob) (*apiv1.Container, error) {
	testContainerName := testjob.Spec.DistributedTest.ContainerName
	for _, container := range testjob.Spec.Template.Spec.Containers {
		if container.Name == testContainerName {
			return &container, nil
		}
	}
	return nil, xerrors.Errorf("cannot find container for running test by name")
}

func (r *TestJobRunner) commandString(c *apiv1.Container) string {
	s := []string{}
	s = append(s, c.Command...)
	s = append(s, c.Args...)
	return strings.Join(s, " ")
}

func (r *TestJobRunner) runDistributedTest(ctx context.Context, testjob TestJob) ([]TestLog, error) {
	testContainer, err := r.testContainer(testjob)
	if err != nil {
		return nil, xerrors.Errorf("failed to find test container: %w", err)
	}
	fmt.Println("get listing of tests...")
	list, err := r.testList(ctx, testjob)
	if err != nil {
		return nil, xerrors.Errorf("failed to get list for testing: %w", err)
	}
	if len(list) == 0 {
		return nil, nil
	}
	plan := r.plan(testjob, list)

	defer func(start time.Time) {
		fmt.Fprintf(os.Stderr, "test: elapsed time %f sec\n", time.Since(start).Seconds())
	}(time.Now())

	failedTestCommands := []*command{}

	var (
		loggerMu      sync.Mutex
		failedTestsMu sync.Mutex
		lastPodIdx    int
	)
	containerNameToLogMap := map[string][]string{}
	podNameToIndexMap := map[string]int{}
	testLogMap := map[string]TestLog{}
	logger := func(log *kubejob.ContainerLog) {
		loggerMu.Lock()
		defer loggerMu.Unlock()

		name := log.Container.Name
		if log.IsFinished {
			cmd, _ := r.containerNameToCommandMap.Load(name)
			logs, exists := containerNameToLogMap[name]
			if exists {
				podName := log.Pod.Name
				idx, exists := podNameToIndexMap[podName]
				if !exists {
					idx = lastPodIdx
					podNameToIndexMap[log.Pod.Name] = lastPodIdx
					lastPodIdx++
				}
				if cmd != nil {
					c := cmd.(*command)
					fmt.Fprintf(os.Stderr, "[POD %d] TEST=%s %s\n", idx, c.test, r.commandString(testContainer))
					testLogMap[c.test] = TestLog{
						Name:           c.test,
						TestResult:     TestResultSuccess,
						ElapsedTimeSec: int(time.Since(c.startedAt).Seconds()),
						Message:        strings.Join(logs, "\n"),
					}
				}
				for _, log := range logs {
					fmt.Fprintf(os.Stderr, "[POD %d] %s", idx, log)
				}
				fmt.Fprintf(os.Stderr, "\n")
			}
			delete(containerNameToLogMap, name)
		} else {
			value, exists := containerNameToLogMap[name]
			logs := []string{}
			if exists {
				logs = value
			} else {
				cmd, _ := r.containerNameToCommandMap.Load(name)
				if cmd != nil {
					startedAt := time.Now()
					for _, status := range log.Pod.Status.ContainerStatuses {
						if log.Container.Name != status.Name {
							continue
						}
						running := status.State.Running
						if running == nil {
							continue
						}
						startedAt = running.StartedAt.Time
					}
					cmd.(*command).startedAt = startedAt
				}
			}
			logs = append(logs, log.Log)
			containerNameToLogMap[name] = logs
		}
	}

	var eg errgroup.Group
	for _, tests := range plan {
		tests := tests
		eg.Go(func() error {
			commands, err := r.runTests(ctx, testjob, logger, testContainer, tests)
			if err != nil {
				return xerrors.Errorf("failed to runTests: %w", err)
			}
			if len(commands) > 0 {
				failedTestsMu.Lock()
				failedTestCommands = append(failedTestCommands, commands...)
				failedTestsMu.Unlock()
			}
			return nil
		})
	}
	if err := eg.Wait(); err != nil {
		return nil, xerrors.Errorf("failed to distributed test job: %w", err)
	}

	if len(failedTestCommands) > 0 {
		for _, command := range failedTestCommands {
			log := testLogMap[command.test]
			log.TestResult = TestResultFailure
			testLogMap[command.test] = log
		}
		logs := []TestLog{}
		for _, log := range testLogMap {
			logs = append(logs, log)
		}
		if !testjob.Spec.DistributedTest.Retest.Enabled {
			return logs, ErrFailedTestJob
		}
		fmt.Println("start retest....")
		tests := []string{}
		for _, command := range failedTestCommands {
			tests = append(tests, command.test)
		}
		concatedTests := strings.Join(tests, testjob.Spec.DistributedTest.Retest.Delimiter)
		failedTests, err := r.runTests(ctx, testjob, logger, testContainer, []string{concatedTests})
		if err != nil {
			return logs, xerrors.Errorf("failed test: %w", err)
		}
		if len(failedTests) > 0 {
			return logs, ErrFailedTestJob
		}
	}
	logs := []TestLog{}
	for _, log := range testLogMap {
		logs = append(logs, log)
	}
	return logs, nil
}

type command struct {
	cmd       []string
	args      []string
	test      string
	container string
	startedAt time.Time
}

type commands []*command

func (c commands) commandValueMap() map[string]*command {
	m := map[string]*command{}
	for _, cc := range c {
		m[cc.test] = cc
	}
	return m
}

func (r *TestJobRunner) testCommand(cmd []string, args []string, test string) *command {
	return &command{
		cmd:  cmd,
		args: args,
		test: test,
	}
}

func (r *TestJobRunner) testsToCommands(c *apiv1.Container, tests []string) commands {
	commands := []*command{}
	for _, test := range tests {
		cmd := r.testCommand(c.Command, c.Args, test)
		commands = append(commands, cmd)
	}
	return commands
}

func (r *TestJobRunner) testContainerWorkingDir(testContainer *apiv1.Container, testjob TestJob) string {
	workingDir := testContainer.WorkingDir
	if workingDir == "" {
		return r.sharedVolumeMount(testjob).MountPath
	}
	return workingDir
}

func (r *TestJobRunner) runTests(ctx context.Context, testjob TestJob, logger kubejob.Logger, testContainer *apiv1.Container, tests []string) ([]*command, error) {
	testCommands := r.testsToCommands(testContainer, tests)
	commandValueMap := testCommands.commandValueMap()

	testContainerWorkingDir := r.testContainerWorkingDir(testContainer, testjob)
	testContainer.WorkingDir = testContainerWorkingDir

	containers := []apiv1.Container{}
	for _, test := range tests {
		container := testContainer.DeepCopy()
		container.Name = ""
		container.Env = append(container.Env, apiv1.EnvVar{
			Name:  "TEST",
			Value: test,
		})
		containers = append(containers, *container)
	}
	job, err := r.newJobForTesting(testjob, containers...)
	if err != nil {
		return nil, err
	}
	for _, cache := range testjob.Spec.DistributedTest.Cache {
		cmd, args := r.command(cache.Command)
		volumeMounts := append(testContainer.VolumeMounts, r.sharedVolumeMount(testjob), apiv1.VolumeMount{
			Name:      cache.Name,
			MountPath: cache.Path,
		})
		cacheContainer := apiv1.Container{
			Name:         cache.Name,
			Image:        testContainer.Image,
			Command:      cmd,
			Args:         args,
			WorkingDir:   testContainerWorkingDir,
			VolumeMounts: volumeMounts,
			Env:          testContainer.Env,
		}
		job.Spec.Template.Spec.Volumes = append(job.Spec.Template.Spec.Volumes, apiv1.Volume{
			Name: cache.Name,
			VolumeSource: apiv1.VolumeSource{
				EmptyDir: &apiv1.EmptyDirVolumeSource{},
			},
		})
		job.Spec.Template.Spec.InitContainers = append(job.Spec.Template.Spec.InitContainers, cacheContainer)
	}
	for i := 0; i < len(testCommands); i++ {
		containerName := job.Spec.Template.Spec.Containers[i].Name
		testCommands[i].container = containerName
		r.containerNameToCommandMap.Store(containerName, testCommands[i])
	}
	job.DisableCommandLog()
	job.SetLogger(logger)
	failedTestCommands := []*command{}
	if err := job.Run(ctx); err != nil {
		var failedJob *kubejob.FailedJob
		if xerrors.As(err, &failedJob) {
			for _, container := range failedJob.FailedContainers() {
				var testName string
				for _, env := range container.Env {
					if env.Name == "TEST" {
						testName = env.Value
						break
					}
				}
				command := commandValueMap[testName]
				failedTestCommands = append(failedTestCommands, command)
			}
		} else {
			return nil, err
		}
	}
	return failedTestCommands, nil
}

func (r *TestJobRunner) testList(ctx context.Context, testjob TestJob) ([]string, error) {
	startListTime := time.Now()
	defer func() {
		fmt.Fprintf(os.Stderr, "list: elapsed time %f sec\n", time.Since(startListTime).Seconds())
	}()
	distributedTest := testjob.Spec.DistributedTest

	listjob := testjob
	container, err := r.testContainer(listjob)
	if err != nil {
		return nil, xerrors.Errorf("failed to find container for list: %w", err)
	}
	container.WorkingDir = r.testContainerWorkingDir(container, listjob)
	container.Command = distributedTest.List.Command
	container.Args = distributedTest.List.Args
	listjob.Spec.Template.Spec.Containers = []apiv1.Container{*container}
	listjob.Spec.Prepare.Steps = []PrepareStepSpec{}
	listjob.Spec.DistributedTest = nil

	listJobRunner := NewTestJobRunner(r.Clientset)
	listJobRunner.DisablePrepareLog()
	listJobRunner.DisableCommandLog()
	listJobRunner.DisableResultLog()

	var pattern *regexp.Regexp
	if distributedTest.List.Pattern != "" {
		reg, err := regexp.Compile(distributedTest.List.Pattern)
		if err != nil {
			return nil, xerrors.Errorf("failed to compile pattern for distributed testing: %w", err)
		}
		pattern = reg
	}

	var b bytes.Buffer
	listJobRunner.SetLogger(func(log *kubejob.ContainerLog) {
		b.WriteString(log.Log)
	})
	if err := listJobRunner.Run(ctx, listjob); err != nil {
		listJobRunner.disabledPrepareLog = false
		listJobRunner.disabledCommandLog = false
		b.Reset()
		listJobRunner.Run(ctx, listjob)
		return nil, xerrors.Errorf("failed to run listJob %s: %w", b.String(), err)
	}
	delim := distributedTest.List.Delimiter
	if delim == "" {
		delim = "\n"
	}
	tests := []string{}
	result := b.String()
	list := strings.Split(result, delim)
	if pattern != nil {
		for _, name := range list {
			if pattern.MatchString(name) {
				tests = append(tests, name)
			}
		}
	} else {
		tests = list
	}
	return tests, nil
}

func (r *TestJobRunner) plan(job TestJob, list []string) [][]string {
	maxContainers := job.Spec.DistributedTest.MaxContainersPerPod

	if len(list) <= maxContainers {
		return [][]string{list}
	}
	concurrent := len(list) / maxContainers
	plan := [][]string{}
	sum := 0
	for i := 0; i <= concurrent; i++ {
		if i == concurrent {
			plan = append(plan, list[sum:])
		} else {
			plan = append(plan, list[sum:sum+maxContainers])
		}
		sum += maxContainers
	}
	return plan
}
